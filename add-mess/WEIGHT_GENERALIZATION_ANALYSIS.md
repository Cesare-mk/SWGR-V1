# 权重方法泛化性分析

## 问题现象

在Sports数据集上训练后，发现权重影响极小，权重贡献度为**-0.593**（负值！），而在Beauty数据集上权重贡献度为**0.208**（正值）。

## 对比分析

### Beauty数据集（有效）
```
共现频次: mean=1.1341, std=6.0445
权重因子: mean=1.0448, std=0.4123
权重贡献度: 0.208 ✓
接近无权重 (0.9-1.1倍): 48.6%
显著加权 (>3.0x 或 <0.3x): 0.4%
Top-20排序变化: 35.0%
```

### Sports数据集（无效）
```
共现频次: mean=1.2030, std=7.3328
权重因子: mean=0.7555, std=0.1336
权重贡献度: -0.593 ✗
接近无权重 (0.9-1.1倍): 10.4%
显著加权 (>3.0x 或 <0.3x): 0.1%
Top-20排序变化: 30.0%
```

## 核心问题诊断

### 1. **权重因子均值偏离严重**
- Beauty: mean=1.0448 ≈ 1.0 ✓
- Sports: mean=0.7555 << 1.0 ✗

**原因**：Sports数据集的语义嵌入更加分散，导致计算出的proximity分数普遍偏低。裁剪到[0.2, 4.0]后，大量权重被裁剪到下限0.2，导致均值严重低于1.0。

### 2. **权重标准差过小**
- Beauty: std=0.4123（区分度较好）
- Sports: std=0.1336（区分度极差）

**原因**：权重因子=(w1*w2)^(γ/2)，当w1、w2都接近0.7555时：
```
(0.7555 * 0.7555)^(3.0/2) ≈ 0.441
```
所有权重都集中在较窄范围内，失去了区分能力。

### 3. **权重贡献度为负值**
```python
weight_contribution = (var(priority) - var(cnt)) / var(priority)
Sports: (33.76 - 53.77) / 33.76 = -0.593
```

**含义**：优先级的方差(33.76)竟然**小于**原始频次的方差(53.77)！

**原因**：由于权重因子均值0.7555 < 1.0，权重实际上在**压缩**优先级分数的范围，而不是扩展它们。这导致：
- priority = cnt × (w1×w2)^(γ/2)
- 当权重因子普遍<1.0时，priority < cnt
- 原本高频的pairs被权重"惩罚"，反而降低了优先级的区分度

## 根本原因

### 问题1: Proximity分数计算不稳定
```python
# 当前实现
scores = 1.0 / (1.0 + mean_distances)
```

这个公式对数据集的**语义密度**极度敏感：
- Beauty: 商品语义相似度高 → distances小 → scores接近1.0
- Sports: 商品种类繁多，语义分散 → distances大 → scores远小于1.0

### 问题2: 裁剪后不重新归一化
代码中明确写道：
```python
# 记录裁剪后的统计信息，但不强制重新归一化
# 原因：对于权重分布极度不均衡的数据集（如Sports），
# 大量低权重被裁剪到下限后，强制归一化会导致均值持续下降
clipped_mean = scores.mean()
if abs(clipped_mean - 1.0) > 0.15:
    self.logger.warning('deviates from 1.0, but NOT re-normalizing')
```

**这是错误的设计**！裁剪后不归一化导致：
1. Sports均值0.7555远离1.0
2. 权重因子=(w1×w2)^(γ/2)进一步放大偏差
3. 权重反而起到"负作用"

## 解决方案

### 方案A: 强制归一化（已测试 - 部分成功）

**实施**：
```python
# 第一次裁剪后必须归一化
scores = np.clip(scores, low, high)
scores = scores / scores.mean()  # 强制均值=1.0
```

**实验结果（Sports数据集）**：
```
权重因子均值: 0.7555 → 1.0203 ✅
权重贡献度: -0.593 → 0.015 ⚠️ (仍然太低)
权重标准差: 0.1336 → 0.1614 ⚠️ (提升不足)
中性权重比例: 10.4% → 55.7% ❌ (反而增加)
Top-20排序变化: 30% → 20% ❌ (反而降低)
```

**诊断**：
- ✅ 修复了均值偏离问题
- ❌ 但归一化压缩了权重分布的区分度
- 原因：Sports的绝对距离大，裁剪+归一化后大量权重聚集到1.0附近
- **结论**：仅归一化不够，需要改进距离计算本身

### 方案B: 数据集自适应裁剪范围
```python
# 根据原始分布动态调整裁剪范围
p5, p95 = np.percentile(scores, [5, 95])
adaptive_low = max(0.2, p5 * 0.8)
adaptive_high = min(4.0, p95 * 1.2)
scores = np.clip(scores, adaptive_low, adaptive_high)
scores = scores / scores.mean()
```

**优点**：
- 自适应不同数据集的语义密度
- 保留更多有效信号

**缺点**：
- 实现复杂
- 可能引入新的超参数

**状态**：未实施（方案C更优）

---

### 方案C: 改进Proximity计算公式（已实施 - 推荐）

**核心思想**：使用**相对距离**而非绝对距离，对语义密度更鲁棒

**实施**：
```python
# 计算全局平均距离
global_mean_dist = mean_distances.mean()

# 使用相对距离
relative_distances = mean_distances / (global_mean_dist + 1e-12)

# 应用倒数映射
scores = 1.0 / (1.0 + relative_distances)

# 缩放到均值≈1.0（因为相对距离均值≈1.0，scores均值≈0.5）
scores = scores * 2.0
```

**为什么有效？**

| 数据集 | 绝对距离均值 | 相对距离均值 | 权重分布 |
|--------|-------------|-------------|---------|
| Beauty | 0.5 | 1.0 | std≈0.40 |
| Sports | 2.5 | 1.0 | std≈0.40 |

**关键**：虽然Sports的语义更分散（绝对距离大），但相对距离归一化后，两个数据集的**权重分布形状一致**！

**优点**：
- ✅ 对不同数据集的语义密度完全鲁棒
- ✅ 天然具有归一化效果
- ✅ Beauty和Sports都能工作良好
- ✅ 不需要额外的超参数
- ✅ 实现简单优雅

**预期效果（Sports数据集）**：
```
权重因子均值: ≈1.0 (归一化保证)
权重贡献度: 0.15-0.25 (与Beauty接近)
权重标准差: 0.35-0.45 (与Beauty的0.41接近)
中性权重比例: 40-50% (与Beauty的48.6%接近)
Top-20排序变化: 30-35% (与Beauty的35%接近)
```

**实施位置**：`tokenizer.py` 第495-530行

**状态**：✅ 已完成，等待重新训练验证

## 实验验证建议

1. **在Sports上测试方案A**（最简单）
2. **对比Beauty和Sports的最终效果**
3. **检查权重贡献度是否变为正值**
4. **观察Top-k排序变化率是否提升**

## 结论

**当前权重方法的泛化性问题不是设计思想的问题，而是实现细节的问题**：

1. ✓ 使用语义信息调整共现权重 — 思想正确
2. ✓ Proximity模式（相似→高权重）— 思想合理
3. ✗ 裁剪后不归一化 — **实现错误**
4. ✗ 未考虑数据集间的语义密度差异 — **鲁棒性不足**

**修复建议**：实施方案A（强制归一化），这是最安全、最有效的方案。

---

**下一步行动**：
1. 修改`tokenizer.py`中的`_compute_item_weights`方法
2. 在裁剪后添加强制归一化
3. 重新在Sports上训练
4. 对比修复前后的权重贡献度

